triton: 
  image: nvcr.io/nvidia/tritonserver:24.06-py3
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          # - key: topology.kubernetes.io/region
          #   operator: In
          #   values:
          #   - us-west
          # - key: nvidia.com/gpu.memory
          #   operator: Gt
          #   values:
          #   - "15000"
          - key: nvidia.com/gpu.product
            operator: In
            values:
            # - NVIDIA-A10
            # - NVIDIA-A40
            # - NVIDIA-A100-SXM4-80GB
            - NVIDIA-L40
            # - NVIDIA-A100-80GB-PCIe
            # - NVIDIA-A100-80GB-PCIe-MIG-1g.10gb
            - NVIDIA-L4
            # - NVIDIA-A100-PCIE-40GB
            # - NVIDIA-GH200-480GB
  args: [tritonserver, 
    --model-repository=/models/icecube.opensciencegrid.org/users/briedel/ml/models, 
    --log-error=true,
    --exit-on-error=true]
  resources:
    limits: { nvidia.com/gpu: 1, cpu: 2, memory: 24Gi }
    requests: { nvidia.com/gpu: 1, cpu: 2, memory: 20Gi }
  modelRepository:
    enabled: true
    storageType: cvmfs-pvc
    mountPath: /models
envoy:
  enabled: true
  auth:
    enabled: true
    jwt_issuer: https://keycloak.icecube.wisc.edu/auth/realms/IceCube
    jwt_remote_jwks_uri: https://keycloak.icecube.wisc.edu/auth/realms/IceCube/protocol/openid-connect/certs
    audiences: [icecube]
    url: keycloak.icecube.wisc.edu
    port: 443
prometheus:
  url: "prometheus.nrp-nautilus.io"
  port: 443
  scheme: https
  serverLoadThreshold: 100
autoscaler:
  enabled: false
  minReplicas: 0
  maxReplicas: 1
ingress:
  enabled: true
  hostName: icesonic.nrp-nautilus.io
