# In the future we might want to change the deployment to allow for 
# getting the A40s, A100s and other "high performance" GPUs in NRP
# we will need create multiple deployments for that to accomondate the 
# NRP setup
# here is an example https://medium.com/@pasternaktal/creating-multiple-deployments-with-different-configurations-using-helm-4992f9f735fd
# multiple deployments can talk to a single service through the tags
# how do we get this to interact with keda properly having multiple 
# deployments is an open question

serverLoadThreshold: 100

nodeSelector:
  topology.kubernetes.io/region: us-west

triton: 
  image: nvcr.io/nvidia/tritonserver:24.10-py3
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          # - key: topology.kubernetes.io/region
          #   operator: In
          #   values:
          #   - us-central
          # - key: nvidia.com/gpu.memory
          #   operator: Gt
          #   values:
          #   - "15000"
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - NVIDIA-A10
            # - NVIDIA-A40
            # - NVIDIA-A100-SXM4-80GB
            # - NVIDIA-L40
            # - NVIDIA-A100-80GB-PCIe
            # - NVIDIA-A100-80GB-PCIe-MIG-1g.10gb
            - NVIDIA-L4
            # - NVIDIA-A100-PCIE-40GB
            # - NVIDIA-GH200-480GB
  args: [tritonserver --model-repository=/models/icecube.opensciencegrid.org/users/briedel/ml/models --log-error=true --exit-on-error=true --buffer-manager-thread-count=128 --pinned-memory-pool-byte-size=12884901888 --cuda-memory-pool-byte-size=0:12884901888 --cuda-virtual-address-size=0:12884901888]
  resources:
    limits: { nvidia.com/gpu: 1, cpu: 8, memory: 24Gi }
    requests: { nvidia.com/gpu: 1, cpu: 6, memory: 20Gi }
  modelRepository:
    enabled: true
    storageType: cvmfs-pvc
    mountPath: /models
  readinessProbe:
    reset: False
    initialDelaySeconds: 300
    periodSeconds: 30
    failureThreshold: 10
    command: ["/bin/sh", "-c", "curl -sf http://localhost:8000/v2/models/tglauch_classifier/versions/3/ready > /dev/null && [ ! -f /tmp/shutdown ]"]
    timeoutSeconds: 5
    successThreshold: 1
  startupProbe:
    reset: False
    initialDelaySeconds: 200
    periodSeconds: 10
    failureThreshold: 10

envoy:
  enabled: true
  replicas: 15
  auth:
    enabled: true
    jwt_issuer: https://keycloak.icecube.wisc.edu/auth/realms/IceCube
    jwt_remote_jwks_uri: https://keycloak.icecube.wisc.edu/auth/realms/IceCube/protocol/openid-connect/certs
    audiences: [icesonic]
    url: keycloak.icecube.wisc.edu
    port: 443
  ingress:
    enabled: true
    hostName: icesonic.nrp-nautilus.io
    ingressClassName: haproxy
    annotations:
      haproxy-ingress.github.io/cors-enable: "true"
      haproxy-ingress.github.io/backend-protocol: "h2"
      haproxy-ingress.github.io/proxy-body-size: "512m"
      haproxy-ingress.github.io/timeout-client: "5m"
      haproxy-ingress.github.io/timeout-server: "5m"
      haproxy-ingress.github.io/timeout-connect: "5m"
      haproxy-ingress.github.io/timeout-http-request: "5m"
      haproxy-ingress.github.io/timeout-queue: "1m"
      haproxy-ingress.github.io/health-check-interval: "30s"
      haproxy-ingress.github.io/health-check-rise-count: "1"

keda:
  enabled: false
  minReplicaCount: 0
  maxReplicaCount: 1

prometheus:
  enabled: true
  external:
    enabled: true
    url: "prometheus.nrp-nautilus.io"
    port: 443
    scheme: https

grafana:
  enabled: true
  dashboardsConfigMaps:
    default: supersonic-grafana-default-dashboard
  datasources:
    datasources.yaml:
      datasources:
        - name: prometheus
          type: prometheus
          access: proxy
          isDefault: true
          url: prometheus.nrp-nautilus.io
          jsonData:
            timeInterval: "5s"
            tlsSkipVerify: true
  ingress:
    enabled: true
    hosts:
      - grafana-icecube.nrp-nautilus.io
    tls:
      - hosts:
          - grafana-icecube.nrp-nautilus.io
    ingressClassName: haproxy
    annotations:
      haproxy-ingress.github.io/cors-enable: "true"
      haproxy-ingress.github.io/backend-protocol: "h2"
      haproxy-ingress.github.io/proxy-body-size: "512m"
  grafana.ini:
    server:
      root_url: https://grafana-icecube.nrp-nautilus.io
