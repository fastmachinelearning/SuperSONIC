# Default configuration values for SuperSONIC

# -- Unique identifier of SuperSONIC instance (equal to release name by default)
nameOverride: ""

# -- A metric used by both KEDA autoscaler and Envoy's prometheus-based rate limiter.
## Default metric (inference queue latency) is defined in templates/_helpers.tpl
serverLoadMetric: ""

# -- Threshold for the metric
serverLoadThreshold: 100

triton:
  # -- Number of Triton server instances (if autoscaling is disabled)
  replicas: 1

  # -- Docker image for the Triton server
  image: "nvcr.io/nvidia/tritonserver:24.12-py3-min"
  
  # -- Command and arguments to run in Triton container
  command: ["/bin/sh", "-c"]
  args: 
    - |
      /opt/tritonserver/bin/tritonserver \
      --model-repository=/tmp/ \
      --log-verbose=0 \
      --exit-timeout-secs=60

  # -- Resource limits and requests for each Triton instance.
  # You can add necessary GPU request here.
  resources:
    limits:
      cpu: 1
      memory: "2G"
    requests:
      cpu: 1
      memory: "2G"

  # -- Annotations for Triton pods
  annotations: {}

  # -- Node selector for Triton pods
  nodeSelector: {}

  # -- Tolerations for Triton pods
  tolerations: []

  # -- Affinity rules for Triton pods - another way to request GPUs
  affinity: {}

  # -- Model repository configuration
  modelRepository:
    enabled: false

    # -- Model repository mount path
    mountPath: ""

    ## Model repository options:

    ## Option 1: mount an arbitrary PersistentVolumeClaim
    # storageType: "pvc"
    # pvc:
    #   claimName: 

    ## Option 2: mount CVMFS as PersistentVolumeClaim (CVMFS StorageClass must be installed at the cluster)
    # storageType: "cvmfs-pvc"
    
    ## Option 3: mount CVMFS via hostPath (CVMFS must be already mounted on the nodes)
    # storageType: "cvmfs"

    ## Option 4: mount an NFS storage volume
    # storageType: "nfs"
    # nfs:
    #   server:
    #   path:

  service:
    labels: {}
    annotations: {}

    # -- Ports for communication with Triton servers
    ports:
      - { name: http, port: 8000, targetPort: 8000, protocol: TCP }
      - { name: grpc, port: 8001, targetPort: 8001, protocol: TCP }
      - { name: metrics, port: 8002, targetPort: 8002, protocol: TCP }

  # -- Custom readiness probe configuration
  readinessProbe:
    # -- If true, will reset settings to k8s defaults (other readinessProbe settings will be ignored)
    reset: false
    initialDelaySeconds: 10
    periodSeconds: 10
    failureThreshold: 10
    command: ["/bin/sh", "-c", "curl -sf http://localhost:8000/v2/health/ready > /dev/null && [ ! -f /tmp/shutdown ]"]
    timeoutSeconds: 5
    successThreshold: 1

  # -- Custom startup probe configuration
  startupProbe:
    # -- If true, will reset settings to k8s defaults (other startupProbe settings will be ignored)
    reset: false
    initialDelaySeconds: 0
    periodSeconds: 10
    failureThreshold: 12
    httpGet:
      path: /v2/health/ready
      port: http
envoy:
  # -- Enable Envoy Proxy
  enabled: true

  # -- Number of Envoy Proxy pods in Deployment
  replicas: 1

  # -- Envoy Proxy Docker image
  image: "envoyproxy/envoy:v1.30.9"

  # -- Arguments for Envoy
  args: ["--config-path", "/etc/envoy/envoy.yaml", "--log-level", "info", "--log-path", "/dev/stdout"]

  # -- Resource requests and limits for Envoy Proxy.
  # Note: an Envoy Proxy with too many connections might run out of CPU
  resources:
    requests:
      cpu: 1.0
      memory: "2G"
    limits:
      cpu: 8.0
      memory: "4G"

  # -- Annotations for Envoy pods
  annotations: {}

  # -- Node selector for Envoy pods
  nodeSelector: {}

  # -- Tolerations for Envoy pods
  tolerations: []

  service:
    # -- This is the client-facing endpoint. In order to be able to connect to it,
    # either enable ingress, or use type: LoadBalancer.
    type: ClusterIP
    # -- Envoy Service ports
    ports:
      - { name: grpc, port: 8001, targetPort: 8001 }
      - { name: admin, port: 9901, targetPort: 9901 }

  # -- Ingress configuration for Envoy
  ingress:
    enabled: false
    hostName: ""
    ingressClassName: ""
    annotations: {}

  # -- Timeout for gRPC route in Envoy; disabled by default (0s), preventing Envoy from closing connections too early.
  grpc_route_timeout: 0s

  rate_limiter:
    # -- This rate limiter explicitly controls the number of client connections to the Envoy Proxy.
    listener_level:
      # -- Enable rate limiter
      enabled: false
      # -- Maximum number of simultaneous connections to the Envoy Proxy.
      # Each new connection takes a "token" from the "bucket" which initially contains ``max_tokens`` tokens.
      max_tokens: 5
      # -- ``tokens_per_fill`` tokens are added to the "bucket" every ``fill_interval``, allowing new connections to be established.
      tokens_per_fill: 1
      # -- For example, adding a new token every 12 seconds allows 5 new connections every minute.
      fill_interval: 12s

    # -- This rate limiter rejects new connections based on metric extracted from Prometheus (e.g. inference queue latency).
    # The metric is taken from parameter ``prometheus.serverLoadMetric``, and the threshold is set by ``prometheus.serverLoadThreshold``.
    # These parameters are the same as those used by the KEDA autoscaler.
    prometheus_based:
      # -- Enable rate limiter
      enabled: false
      luaConfig: "cfg/envoy-filter.lua" 

  # -- Envoy load balancer policy.
  # Options: ROUND_ROBIN, LEAST_REQUEST, RING_HASH, RANDOM, MAGLEV
  loadBalancerPolicy: "LEAST_REQUEST"

  auth:
    # -- Enable authentication in Envoy proxy
    enabled: false
    jwt_issuer: ""
    jwt_remote_jwks_uri: ""
    audiences: []
    url: ""
    port: 443

autoscaler:

  # -- Enable autoscaling (requires Prometheus to also be enabled).
  # Autoscaling will be based on the metric is taken from parameter ``prometheus.serverLoadMetric``,
  # new Triton servers will spawn if the metric exceedds the threshold set by ``prometheus.serverLoadThreshold``.
  enabled: false

  # -- Minimum and maximum number of Triton servers.
  # Warning: if min=0 and desired Prometheus metric is empty, the first server will never start
  minReplicaCount: 1
  maxReplicaCount: 2

  # -- If set to true, the server will release all GPUs when idle.
  # Be careful: if the scaling metric is extracted from Triton servers,
  # it will be unavailable, and scaling from 0 to 1 will never happen.
  zeroIdleReplicas: false

  scaleUp:
    stabilizationWindowSeconds: 60
    periodSeconds: 60
    stepsize: 1
  scaleDown:
    stabilizationWindowSeconds: 600
    periodSeconds: 120
    stepsize: 1

prometheus:
  external:
    # -- Enable external Prometheus instance. If true, Prometheus parameters outside of prometheus.external will be ignored.
    enabled: false
    # -- External Prometheus server url
    url: ""
    # -- External Prometheus server port number
    port: 443
    # -- Specify whether external Prometheus endpoint is exposed as http or https
    scheme: "https"

  # -- Enable or disable custom Prometheus deployment
  enabled: false

  server:
    extraFlags: [web.enable-remote-write-receiver]
    useExistingClusterRoleName: supersonic-prometheus-role # If installer plugin is used, this value is set by the plugin
    releaseNamespace: true
    persistentVolume:
      enabled: false
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1
        memory: 1Gi
    retention: 15d
    global:
      scrape_interval: 5s
      evaluation_interval: 5s
    service:
      enabled: true
      servicePort: 9090
    configMapOverrideName: prometheus-config

    # -- Ingress configuration for Prometheus
    ingress:
      enabled: false
      # -- If this parameter is set in values.yaml, the `hosts` and `tls` parameters
      # can be ommitted, as they will be set by the installer plugin
      hostName: ""
      hosts: []
      tls:
        - hosts: []
      ingressClassName: ""
      annotations: {}

  serviceAccounts:
    server:
      create: false
      name: supersonic-prometheus-sa # If installer plugin is used, this value is set by the plugin

  rbac:
    create: false
  alertmanager:
    enabled: false
  pushgateway:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  configmapReload:
    prometheus:
      enabled: false

## Grafana Helm Chart Configuration
## Configuration for the official Grafana Helm chart (https://github.com/grafana/helm-charts)
grafana:
  # -- Enable Grafana
  enabled: false
  adminUser: admin
  adminPassword: admin
  persistence:
    enabled: false
  rbac:
    create: false
  serviceAccount:
    create: false

  # -- Grafana datasources configuration
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      # If installer plugin is used, this section is set by the plugin
        - name: prometheus
          type: prometheus
          access: proxy
          isDefault: true
          url: http://supersonic-prometheus-server:9090
          jsonData:
            timeInterval: "5s"
            tlsSkipVerify: true
        - name: tempo
          type: tempo
          url: http://supersonic-tempo:3100
          access: proxy
          isDefault: false
          basicAuth: false
          jsonData:
            timeInterval: "5s"
            tlsSkipVerify: true
            serviceMap:
              datasourceUid: 'prometheus'
            nodeGraph:
              enabled: true


  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default

  dashboardsConfigMaps:
    default: supersonic-grafana-default-dashboard # If installer plugin is used, this value is set by the plugin

  grafana.ini:
    auth:
      disable_login_form: true
    auth.anonymous:
      enabled: true
      org_role: Admin
    dashboards:
      default_home_dashboard_path: /var/lib/grafana/dashboards/default/default.json
    server:
      root_url: "" # If installer plugin is used, this value is set by the plugin

  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 128Mi

  # -- Ingress configuration for Grafana
  ingress:
    enabled: false
    path: /
    pathType: ImplementationSpecific
    ingressClassName: ""
    # -- If this parameter is set in values.yaml, the `hosts` and `tls` parameters
    # can be ommitted, as they will be set by the installer plugin
    hostName: ""
    hosts: [] # If installer plugin is used, this value is set by the plugin
    tls:
      - hosts: [] # If installer plugin is used, this value is set by the plugin
    annotations: {}

# Tempo configuration
tempo:
  enabled: false
  tempo:
    metricsGenerator:
      enabled: true
      remoteWriteUrl: http://supersonic-prometheus-server:9090/api/v1/write
    resources:
      requests:
        cpu: "1"
        memory: "2G"
      limits:
        cpu: "2"
        memory: "4G"
    overrides:
      defaults:
        metrics_generator_processors:
          - 'service-graphs'
          - 'span-metrics'
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
    livenessProbe:
      initialDelaySeconds: 0
    readinessProbe:
      initialDelaySeconds: 0

tracing_sampling_rate: 0.01

opentelemetry-collector:
  enabled: false
  image:
    repository: "otel/opentelemetry-collector-contrib"
    tag: "0.120.0"
  mode: deployment
  resources:
    requests:
      memory: "1G"
      cpu: "1"
    limits:
      memory: "2G"
      cpu: "1"
  ports:
    metrics:
      enabled: true
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
    exporters:
      otlp:
        endpoint: http://supersonic-tempo:4317
        tls:
          insecure: true
      otlphttp:
        endpoint: http://supersonic-tempo:4318
        tls:
          insecure: true
      prometheusremotewrite:
        endpoint: http://supersonic-prometheus-server:9090/api/v1/write
        target_info:
          enabled: true
    processors:
      transform:
        trace_statements:
        - context: resource
          conditions:
            - 'attributes["pod_name"] != nil'
          statements:
            - replace_match(attributes["service.name"], "triton-inference-server", attributes["pod_name"])
        - context: span
          conditions:
            - 'name == "ingress"'
          statements:
            - set(kind, 3)
            - set(kind.string, "Client")
        - context: span
          conditions:
            - name != "ingress"
          statements:
            - set(kind, 2)
            - set(kind.string, "Server")
    connectors:
      spanmetrics:
        histogram:
          explicit:
            buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]
        dimensions:
          - name: http.method
            default: GET
          - name: http.status_code
        exemplars:
          enabled: true
        exclude_dimensions: ['status.code']
        dimensions_cache_size: 1000
        aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"    
        metrics_flush_interval: 15s
        metrics_expiration: 5m
        events:
          enabled: true
          dimensions:
            - name: exception.type
            - name: exception.message
        resource_metrics_key_attributes:
          - service.name
          - telemetry.sdk.language
          - telemetry.sdk.name
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [transform]
          exporters: [otlp, spanmetrics]
        metrics:
          receivers: [spanmetrics]
          processors: [batch]
          exporters: [prometheusremotewrite]

metricsCollector:
  # -- Enable metrics collector
  enabled: false

  # -- Host and port for metrics collector
  host: "0.0.0.0"
  port: 8003

  # -- Resource limits and requests for metrics collector
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 512Mi

  # -- Add service
  service:
    port: 8003
    type: ClusterIP

  # -- Ingress configuration for metrics collector
  ingress:
    enabled: false
    hostName: ""
    ingressClassName: ""
    annotations: {}

