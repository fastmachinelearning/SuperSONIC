# Default configuration values for SuperSONIC

# -- Unique identifier of SuperSONIC instance (equal to release name by default)
nameOverride: ""

# -- A metric used by both KEDA autoscaler and Envoy's prometheus-based rate limiter.
## Default metric (inference queue latency) is defined in templates/_helpers.tpl
serverLoadMetric: ""

# -- Threshold for the metric
serverLoadThreshold: 100

triton:
  # -- Number of Triton server instances (if autoscaling is disabled)
  replicas: 1

  # -- Docker image for the Triton server
  image: "nvcr.io/nvidia/tritonserver:24.12-py3-min"
  
  # -- Command and arguments to run in Triton container
  command: ["/bin/sh", "-c"]
  args: 
    - |
      /opt/tritonserver/bin/tritonserver \
      --model-repository=/tmp/ \
      --log-verbose=0 \
      --exit-timeout-secs=60

  # -- Resource limits and requests for each Triton instance.
  # You can add necessary GPU request here.
  resources:
    limits:
      cpu: 1
      memory: "2G"
    requests:
      cpu: 1
      memory: "2G"
  
  # -- Affinity rules for Triton pods - another way to request GPUs
  affinity: {}

  # -- Model repository configuration
  modelRepository:
    enabled: false

    # -- Model repository mount path
    mountPath: ""

    ## Model repository options:

    ## Option 1: mount an arbitrary PersistentVolumeClaim
    # storageType: "pvc"
    # pvc:
    #   claimName: 

    ## Option 2: mount CVMFS as PersistentVolumeClaim (CVMFS StorageClass must be installed at the cluster)
    # storageType: "cvmfs-pvc"
    
    ## Option 3: mount CVMFS via hostPath (CVMFS must be already mounted on the nodes)
    # storageType: "cvmfs"

    ## Option 4: mount an NFS storage volume
    # storageType: "nfs"
    # nfs:
    #   server:
    #   path:

  service:
    labels: {}
    annotations: {}

    # -- Ports for communication with Triton servers
    ports:
      - { name: http, port: 8000, targetPort: 8000, protocol: TCP }
      - { name: grpc, port: 8001, targetPort: 8001, protocol: TCP }
      - { name: metrics, port: 8002, targetPort: 8002, protocol: TCP }

  # -- If ture, will ignore custom readinness probe settings (not recommended when using autoscaler)
  resetReadinessProbe: false

envoy:
  # -- Enable Envoy Proxy
  enabled: true

  # -- Number of Envoy Proxy pods in Deployment
  replicas: 1

  # -- Envoy Proxy Docker image
  image: "envoyproxy/envoy:v1.30-latest"

  # -- Arguments for Envoy
  args: ["--config-path", "/etc/envoy/envoy.yaml", "--log-level", "info", "--log-path", "/dev/stdout"]

  # -- Resource requests and limits for Envoy Proxy.
  # Note: an Envoy Proxy with too many connections might run out of CPU
  resources:
    requests:
      cpu: 1
      memory: "2G"
    limits:
      cpu: 2
      memory: "4G"
  service:
    # -- This is the client-facing endpoint. In order to be able to connect to it,
    # either enable ingress, or use type: LoadBalancer.
    type: ClusterIP
    # -- Envoy Service ports
    ports:
      - { name: grpc, port: 8001, targetPort: 8001 }
      - { name: admin, port: 9901, targetPort: 9901 }

  # -- Ingress configuration for Envoy
  ingress:
    enabled: false
    hostName: ""
    ingressClassName: ""
    annotations: {}

  # -- Timeout for gRPC route in Envoy; disabled by default (0s), preventing Envoy from closing connections too early.
  grpc_route_timeout: 0s

  rate_limiter:
    # -- This rate limiter explicitly controls the number of client connections to the Envoy Proxy.
    listener_level:
      # -- Enable rate limiter
      enabled: false
      # -- Maximum number of simultaneous connections to the Envoy Proxy.
      # Each new connection takes a "token" from the "bucket" which initially contains ``max_tokens`` tokens.
      max_tokens: 5
      # -- ``tokens_per_fill`` tokens are added to the "bucket" every ``fill_interval``, allowing new connections to be established.
      tokens_per_fill: 1
      # -- For example, adding a new token every 12 seconds allows 5 new connections every minute.
      fill_interval: 12s

    # -- This rate limiter rejects new connections based on metric extracted from Prometheus (e.g. inference queue latency).
    # The metric is taken from parameter ``prometheus.serverLoadMetric``, and the threshold is set by ``prometheus.serverLoadThreshold``.
    # These parameters are the same as those used by the KEDA autoscaler.
    prometheus_based:
      # -- Enable rate limiter
      enabled: false
      luaConfig: "cfg/envoy-filter.lua" 

  # -- Envoy load balancer policy.
  # Options: ROUND_ROBIN, LEAST_REQUEST, RING_HASH, RANDOM, MAGLEV
  loadBalancerPolicy: "LEAST_REQUEST"

  auth:
    # -- Enable authentication in Envoy proxy
    enabled: false
    jwt_issuer: ""
    jwt_remote_jwks_uri: ""
    audiences: []
    url: ""
    port: 443

autoscaler:

  # -- Enable autoscaling (requires Prometheus to also be enabled).
  # Autoscaling will be based on the metric is taken from parameter ``prometheus.serverLoadMetric``,
  # new Triton servers will spawn if the metric exceedds the threshold set by ``prometheus.serverLoadThreshold``.
  enabled: false

  # -- Minimum and maximum number of Triton servers.
  # Warning: if min=0 and desired Prometheus metric is empty, the first server will never start
  minReplicas: 1
  maxReplicas: 2

  # -- If set to true, the server will release all GPUs when idle.
  # Be careful: if the scaling metric is extracted from Triton servers,
  # it will be unavailable, and scaling from 0 to 1 will never happen.
  zeroIdleReplicas: false

  scaleUp:
    window: 60
    period: 60
    stepsize: 1
  scaleDown:
    window: 600
    period: 120
    stepsize: 1

# -- Node selector for all pods (Triton and Envoy)
nodeSelector: {}

# -- Tolerations for all pods (Triton and Envoy)
tolerations: []

# -- Connection to a Prometheus server is required for KEDA autoscaler and Envoy's prometheus-based rate limiter
prometheus:
  external:
    # -- Enable external Prometheus instance
    enabled: false
    # -- External Prometheus server url
    url: ""
    # -- External Prometheus server port number
    port: 443
    # -- Specify whether external Prometheus endpoint is exposed as http or https
    scheme: "https"

  # -- Enable or disable Prometheus subchart deployment
  enabled: false

  # -- Prometheus Helm chart configuration (https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus)
  server:
    useExistingClusterRoleName: supersonic-prometheus-role
    releaseNamespace: true
    persistentVolume:
      enabled: false
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1
        memory: 1Gi
    retention: 15d
    global:
      scrape_interval: 5s
      evaluation_interval: 5s
    service:
      enabled: true
      servicePort: 9090
    configMapOverrideName: prometheus-config
    ingress:
      enabled: false
      hosts: []
      ingressClassName: ""
      annotations: {}
      tls:
        - hosts: []

  serviceAccounts:
    server:
      create: false
      name: supersonic-prometheus-sa

  rbac:
    create: false
  alertmanager:
    enabled: false
  pushgateway:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  configmapReload:
    prometheus:
      enabled: false

## Grafana Helm Chart Configuration
## Configuration for the official Grafana Helm chart (https://github.com/grafana/helm-charts)
grafana:
  enabled: false
  adminUser: admin
  adminPassword: admin
  persistence:
    enabled: false
  rbac:
    create: false
  serviceAccount:
    create: false

  # -- Grafana datasources configuration
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: prometheus
          type: prometheus
          access: proxy
          isDefault: true
          url: http://supersonic-prometheus-server:9090
          jsonData:
            timeInterval: "5s"
            tlsSkipVerify: true

  # -- Grafana dashboard providers configuration
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default

  # -- Grafana dashboard ConfigMaps
  dashboardsConfigMaps:
    default: supersonic-grafana-default-dashboard

  # -- Grafana.ini configuration
  grafana.ini:
    auth:
      disable_login_form: true
    auth.anonymous:
      enabled: true
      org_role: Admin
    dashboards:
      default_home_dashboard_path: /var/lib/grafana/dashboards/default/default.json

  # -- Resource limits and requests for Grafana
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 128Mi

  # -- Service configuration
  service:
    type: ClusterIP
    port: 80
    targetPort: 3000

  # -- Ingress configuration
  ingress:
    enabled: false
    path: /
    pathType: ImplementationSpecific
    ingressClassName: ""
    hosts: []
    tls: []
    annotations: {}